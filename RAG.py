# from langchain.document_loaders import PyPDFLoader
import os
import io
import pandas as pd
from dotenv import load_dotenv
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
import streamlit as st
import time
import re

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_ollama import ChatOllama
from langchain import hub
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.document_loaders import CSVLoader
from langchain_chroma import Chroma
from chromadb.config import Settings
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import warnings
warnings.filterwarnings("ignore", category=UserWarning)  # torch ê´€ë ¨ ê²½ê³  ë¬´ì‹œ
load_dotenv()

import nltk
nltk.download('punkt')

# ì¸ì¦ íŒŒì¼ ê²½ë¡œ ë° ê¶Œí•œ ì„¤ì •
SERVICE_ACCOUNT_FILE = 'credentials.json'
SCOPES = ['https://www.googleapis.com/auth/drive.readonly']

# ì‚¬ìš©ì í”¼ë“œë°± ì €ì¥ì„ ìœ„í•œ íŒŒì¼ ê²½ë¡œ
FEEDBACK_FILE = "feedback_log.csv"

# ì‚¬ìš©ì í”¼ë“œë°± ì €ì¥ í•¨ìˆ˜
def save_feedback(query, output, feedback):
    feedback_data = {
        "query": query,
        "output": output,
        "feedback": feedback
    }
    try:
        if not os.path.exists(FEEDBACK_FILE):
            pd.DataFrame([feedback_data]).to_csv(FEEDBACK_FILE, index=False)
            print(f"[INFO] í”¼ë“œë°± íŒŒì¼ ìƒì„±: {FEEDBACK_FILE}")
        else:
            existing_data = pd.read_csv(FEEDBACK_FILE)
            updated_data = pd.concat([existing_data, pd.DataFrame([feedback_data])], ignore_index=True)
            updated_data.to_csv(FEEDBACK_FILE, index=False)
            print(f"[INFO] í”¼ë“œë°± íŒŒì¼ ì—…ë°ì´íŠ¸: {FEEDBACK_FILE}")
    except Exception as e:
        print(f"[ERROR] í”¼ë“œë°± ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì¸ì¦
def authenticate_drive():
    creds = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)
    service = build('drive', 'v3', credentials=creds)
    return service

# êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„œ íŠ¹ì • mimeTypeì˜ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
def get_files(service, folder_id, mime_type):
    query = f"'{folder_id}' in parents and mimeType = '{mime_type}'"
    results = service.files().list(q=query).execute()
    return results.get('files', [])

# íŒŒì¼ì„ ë©”ëª¨ë¦¬ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë°˜í™˜
def download_file_to_memory(service, file_id):
    request = service.files().get_media(fileId=file_id)
    fh = io.BytesIO()
    downloader = MediaIoBaseDownload(fh, request)
    while not downloader.next_chunk()[1]:
        pass
    fh.seek(0)
    return fh

# CSV íŒŒì¼ ë³‘í•©
# íŠ¹ìˆ˜ë¬¸ì ë° í° ê³µë°± ì²˜ë¦¬ ì¶”ê°€
def merge_csv(files, service):
    dfs = []
    for file in files:
        df = pd.read_csv(download_file_to_memory(service, file['id']))
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° í° ê³µë°± ì²˜ë¦¬
        for column in df.select_dtypes(include=['object']).columns:
            df[column] = df[column].str.replace(r'[^\w\s]', '', regex=True)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
            df[column] = df[column].str.replace(r'\s+', ' ', regex=True).str.strip()  # í° ê³µë°± ì œê±°
        dfs.append(df)

    merged_df = pd.concat(dfs, ignore_index=True)
    merged_df = merged_df.drop_duplicates(subset=['title'])  # content ì—´ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
    merged_df.to_csv('merged_data.csv', index=False)
    print("CSV files merged successfully with special character and whitespace handling.")

# CSV íŒŒì¼ ë¡œë“œ ë° í…ìŠ¤íŠ¸ ë¶„í• 
def load_and_split_csv(file_path, column_name="content", chunk_size=1000, chunk_overlap=100):
    loader = CSVLoader(file_path=file_path, source_column=column_name, encoding="utf-8")
    pages = loader.load_and_split()

    # ë™ì  chunk_sizeì™€ chunk_overlap ì ìš©
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    docs = text_splitter.split_documents(pages)
    return docs

# ë²¡í„° ì €ì¥ì†Œ ìƒì„±
def create_vectorstore(docs):
    model_name = "jhgan/ko-sbert-nli"
    model_kwargs = {'device': 'cuda'}
    encode_kwargs = {'normalize_embeddings': True}

    embedding = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )

    # Chroma ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
    vectorstore = Chroma.from_documents(
        docs,
        embedding,
        persist_directory="./chroma_db"  # settings ëŒ€ì‹  persist_directory ì‚¬ìš©
    )
    return vectorstore.as_retriever()

# í…ìŠ¤íŠ¸ í˜•ì‹í™”
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# ì„ë² ë”© ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í‰ê°€ (ì—¬ëŸ¬ ë¬¸ë§¥ ì‚¬ìš©)
def evaluate_with_embedding(output, contexts):
    """
    ì¶œë ¥ ê²°ê³¼ì™€ ì—¬ëŸ¬ ë¬¸ë§¥(contexts)ì˜ ì„ë² ë”© ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    model_name = "jhgan/ko-sbert-nli"
    embedding_model = HuggingFaceEmbeddings(model_name=model_name)

    # ì¶œë ¥ ê²°ê³¼ ì„ë² ë”© ê³„ì‚°
    output_embedding = embedding_model.embed_query(output)

    # ê° ë¬¸ë§¥ì˜ ì„ë² ë”© ê³„ì‚° ë° ìœ ì‚¬ë„ ì¸¡ì •
    similarities = []
    for context in contexts:
        context_embedding = embedding_model.embed_query(context)
        similarity = cosine_similarity(
            [output_embedding], [context_embedding]
        )[0][0]
        similarities.append(similarity)

    # ê°€ì¥ ë†’ì€ ìœ ì‚¬ë„ë¥¼ ë°˜í™˜
    return max(similarities) if similarities else 0.0

# RAG ì²´ì¸ ì„¤ì • ë° ì‹¤í–‰
def run_rag_chain(retriever, query):
    prompt = hub.pull("rlm/rag-prompt")
    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | ChatOllama(model="EEVE-Korean-10.8B:latest")
        | StrOutputParser()
    )

    # ì¶œë ¥ ê²°ê³¼ ì €ì¥ ë° ë¬¸ë§¥ ê°€ì ¸ì˜¤ê¸°
    output = ""
    context_docs = retriever.get_relevant_documents(query)
    contexts = [doc.page_content for doc in context_docs]
    for chunk in rag_chain.stream(query):
        output += chunk

    # ì—¬ëŸ¬ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì„ë² ë”© ìœ ì‚¬ë„ ê³„ì‚°
    score = evaluate_with_embedding(output, contexts)

    return output, score, contexts

def main():
    # Streamlit ë ˆì´ì•„ì›ƒ ì„¤ì •
    st.set_page_config(
        page_title="RAG ê¸°ë°˜ ì±—ë´‡",
        page_icon="ğŸ¤–",
        layout="wide"  # ë³¸ë¬¸ì„ ë„“ê²Œ ì„¤ì •
    )

    st.title("ğŸ¤– RAG ê¸°ë°˜ ì±—ë´‡")

    # êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì¸ì¦ ë° CSV íŒŒì¼ ì²˜ë¦¬
    FOLDER_ID = os.getenv('FOLDER_ID')
    service = authenticate_drive()
    csv_files = get_files(service, FOLDER_ID, 'text/csv')

    # ë³‘í•© ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„± ìƒíƒœë¥¼ ì¶”ì í•˜ê¸° ìœ„í•œ session_state ì„¤ì •
    if 'csv_merged' not in st.session_state:
        st.session_state['csv_merged'] = False
    if 'vectorstore_created' not in st.session_state:
        st.session_state['vectorstore_created'] = False

    # ì‚¬ì´ë“œë°”ì— CSV íŒŒì¼ ëª©ë¡ì„ ì—°ë„, ì›”ë³„ íŠ¸ë¦¬ í˜•ì‹ìœ¼ë¡œ í‘œì‹œ
    st.sidebar.title("ğŸ“‚ CSV íŒŒì¼ ëª©ë¡ & ë‹¤ìš´ë¡œë“œ")
    if csv_files:
        file_tree = {}

        # íŒŒì¼ ì´ë¦„ì—ì„œ ë‚ ì§œ ì¶”ì¶œ ë° íŠ¸ë¦¬ êµ¬ì¡° ìƒì„±
        for file in csv_files:
            file_name = file['name']
            file_id = file['id']
            file_content = download_file_to_memory(service, file_id).getvalue()

            # ë‚ ì§œ ì¶”ì¶œ (ì˜ˆ: 2023-04-10 í˜•ì‹)
            date_match = re.search(r'(\d{4})-(\d{2})-(\d{2})', file_name)
            if date_match:
                year, month, _ = date_match.groups()
                if year not in file_tree:
                    file_tree[year] = {}
                if month not in file_tree[year]:
                    file_tree[year][month] = []
                file_tree[year][month].append((file_name, file_id, file_content))

        # íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ ì‚¬ì´ë“œë°”ì— í‘œì‹œ (st.session_state ì œê±°)
        for year, months in sorted(file_tree.items()):
            with st.sidebar.expander(f"ğŸ“ {year}"):
                for month, files in sorted(months.items()):
                    st.markdown(f"### ğŸ“‚ {month}ì›”")
                    for file_name, file_id, file_content in files:
                        st.download_button(
                            label=f"ğŸ“¥ {file_name}",
                            data=file_content,
                            file_name=file_name,
                            mime="text/csv",
                            key=f"download-{file_id}"  # ê³ ìœ í•œ í‚¤ ì¶”ê°€
                        )
    else:
        st.sidebar.warning("ğŸš¨ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    if csv_files and not st.session_state['csv_merged']:
        with st.spinner("â³ CSV íŒŒì¼ ë³‘í•© ì¤‘..."):
            progress_bar = st.progress(0)
            total_files = len(csv_files)
            for i, file in enumerate(csv_files):
                # ì‹¤ì œ ì²˜ë¦¬ ë¡œì§ (ì˜ˆ: íŒŒì¼ ì½ê¸° ë° ë³‘í•©)
                pd.read_csv(download_file_to_memory(service, file['id']))
                progress = int(((i + 1) / total_files) * 100)  # ì§„í–‰ë¥  ê³„ì‚°
                progress_bar.progress(progress)
            merge_csv(csv_files, service)
        st.success("âœ… CSV íŒŒì¼ì´ ë³‘í•©ë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.session_state['csv_merged'] = True

    # ë³‘í•©ëœ CSV íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°
    if st.session_state['csv_merged'] and not st.session_state['vectorstore_created']:
        try:
            merged_data = pd.read_csv("merged_data.csv")
            st.subheader("ğŸ“Š ë³‘í•©ëœ CSV ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
            st.dataframe(merged_data[['title', 'date', 'summary']].head())  # title, date, summary ì—´ë§Œ ìƒìœ„ 10ê°œ í–‰ í‘œì‹œ
        except Exception as e:
            st.error(f"ğŸš¨ ë³‘í•©ëœ CSV íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

        # CSV íŒŒì¼ ë¡œë“œ ë° ë²¡í„°í™”
        try:
            with st.spinner("â³ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘..."):
                docs = load_and_split_csv("merged_data.csv")
                progress_bar = st.progress(0)
                for i in range(20):  # ë²¡í„°í™” ì‹œë®¬ë ˆì´ì…˜
                    time.sleep(0.5)
                    progress_bar.progress((i + 1) * 5)
                retriever = create_vectorstore(docs)
                st.session_state['retriever'] = retriever  # retrieverë¥¼ session_stateì— ì €ì¥
            st.success("âœ… ë²¡í„° ì €ì¥ì†Œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.session_state['vectorstore_created'] = True
        except Exception as e:
            st.error(f"ğŸš¨ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            return  # ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì‹¤íŒ¨ ì‹œ ì‹¤í–‰ ì¤‘ë‹¨

    if st.session_state['vectorstore_created']:
        retriever = st.session_state.get('retriever', None)
        if retriever is None:
            st.error("ğŸš¨ RAG ì²´ì¸ì„ ì‹¤í–‰í•˜ê¸° ìœ„í•´ retrieverê°€ í•„ìš”í•©ë‹ˆë‹¤. ë²¡í„° ì €ì¥ì†Œ ìƒì„± ê³¼ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
            return

        query = st.text_input("ğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")

        if st.button("ğŸš€ ì§ˆë¬¸ ì‹¤í–‰"):
            if query:
                if retriever is None:
                    st.error("ğŸš¨ RAG ì²´ì¸ì„ ì‹¤í–‰í•˜ê¸° ìœ„í•´ retrieverê°€ í•„ìš”í•©ë‹ˆë‹¤. ë²¡í„° ì €ì¥ì†Œ ìƒì„± ê³¼ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
                    return
                with st.spinner("â³ RAG ì²´ì¸ì„ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤..."):
                    try:
                        output, score, contexts = run_rag_chain(retriever, query)
                        st.success("âœ… RAG ì²´ì¸ ì‹¤í–‰ ì™„ë£Œ!")
                        st.text_area("ğŸ“œ ì¶œë ¥ ê²°ê³¼", output, height=400)
                        st.write(f"**[ğŸŒŸ ì„ë² ë”© ìœ ì‚¬ë„]:** {score:.3f}")

                        # í”¼ë“œë°± ë²„íŠ¼ ì¶”ê°€
                        col1, col2 = st.columns(2)
                        with col1:
                            if st.button("ğŸ‘ ìœ ìš©í•¨", key="positive_feedback"):
                                save_feedback(query, output, "positive")
                                st.success("í”¼ë“œë°±ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
                        with col2:
                            if st.button("ğŸ‘ ìœ ìš©í•˜ì§€ ì•ŠìŒ", key="negative_feedback"):
                                save_feedback(query, output, "negative")
                                st.warning("í”¼ë“œë°±ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")

                        # ë¬¸ë§¥ í•˜ì´ë¼ì´íŠ¸ ì¶”ê°€
                        st.write("ğŸ” ê´€ë ¨ ë¬¸ë§¥:")
                        for context in contexts:
                            highlighted_context = context.replace(query, f"**{query}**")  # ê°„ë‹¨í•œ í•˜ì´ë¼ì´íŠ¸
                            st.markdown(f"{highlighted_context}")

                    except Exception as e:
                        st.error(f"ğŸš¨ RAG ì²´ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            else:
                st.error("ğŸš¨ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        retriever = None

if __name__ == '__main__':
    main()
