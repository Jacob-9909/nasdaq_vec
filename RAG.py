# from langchain.document_loaders import PyPDFLoader
import os
import io
import pandas as pd
from dotenv import load_dotenv
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
import streamlit as st
import time
import re

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_ollama import ChatOllama
from langchain import hub
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.document_loaders import CSVLoader
from langchain_chroma import Chroma
from chromadb.config import Settings
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import warnings
warnings.filterwarnings("ignore", category=UserWarning)  # torch ê´€ë ¨ ê²½ê³  ë¬´ì‹œ
load_dotenv()

import nltk
nltk.download('punkt')

# ì¸ì¦ íŒŒì¼ ê²½ë¡œ ë° ê¶Œí•œ ì„¤ì •
SERVICE_ACCOUNT_FILE = 'credentials.json'
SCOPES = ['https://www.googleapis.com/auth/drive.readonly']

# ì‚¬ìš©ì í”¼ë“œë°± ì €ì¥ì„ ìœ„í•œ íŒŒì¼ ê²½ë¡œ
FEEDBACK_FILE = "feedback_log.csv"

# ì‚¬ìš©ì í”¼ë“œë°± ì €ì¥ í•¨ìˆ˜
def save_feedback(query, output, feedback):
    from datetime import datetime
    
    feedback_data = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "query": str(query),
        "output": str(output),
        "feedback": str(feedback)
    }
    
    try:
        if not os.path.exists(FEEDBACK_FILE):
            # ìƒˆ íŒŒì¼ ìƒì„±
            pd.DataFrame([feedback_data]).to_csv(FEEDBACK_FILE, index=False, encoding='utf-8-sig')
            st.success(f"í”¼ë“œë°±ì´ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            try:
                # ê¸°ì¡´ íŒŒì¼ ì½ê¸°
                existing_data = pd.read_csv(FEEDBACK_FILE, encoding='utf-8-sig')
                updated_data = pd.concat([existing_data, pd.DataFrame([feedback_data])], ignore_index=True)
                # ì„ì‹œ íŒŒì¼ë¡œ ë¨¼ì € ì €ì¥
                temp_file = FEEDBACK_FILE + '.tmp'
                updated_data.to_csv(temp_file, index=False, encoding='utf-8-sig')
                # ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ë©´ ì›ë³¸ íŒŒì¼ êµì²´
                os.replace(temp_file, FEEDBACK_FILE)
                st.success(f"í”¼ë“œë°±ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
            except pd.errors.EmptyDataError:
                # ë¹ˆ íŒŒì¼ì¸ ê²½ìš° ìƒˆë¡œ ìƒì„±
                pd.DataFrame([feedback_data]).to_csv(FEEDBACK_FILE, index=False, encoding='utf-8-sig')
                st.success(f"í”¼ë“œë°±ì´ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        error_msg = f"í”¼ë“œë°± ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(f"[ERROR] {error_msg}")
        st.error(error_msg)
        # ì„ì‹œ íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ì‚­ì œ
        if os.path.exists(FEEDBACK_FILE + '.tmp'):
            os.remove(FEEDBACK_FILE + '.tmp')

# êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì¸ì¦
def authenticate_drive():
    creds = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)
    service = build('drive', 'v3', credentials=creds)
    return service

# êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„œ íŠ¹ì • mimeTypeì˜ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
def get_files(service, folder_id, mime_type):
    query = f"'{folder_id}' in parents and mimeType = '{mime_type}'"
    results = service.files().list(q=query).execute()
    return results.get('files', [])

# íŒŒì¼ì„ ë©”ëª¨ë¦¬ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë°˜í™˜
def download_file_to_memory(service, file_id):
    request = service.files().get_media(fileId=file_id)
    fh = io.BytesIO()
    downloader = MediaIoBaseDownload(fh, request)
    while not downloader.next_chunk()[1]:
        pass
    fh.seek(0)
    return fh

# CSV íŒŒì¼ ë³‘í•©
# íŠ¹ìˆ˜ë¬¸ì ë° í° ê³µë°± ì²˜ë¦¬ ì¶”ê°€
def merge_csv(files, service):
    dfs = []
    for file in files:
        df = pd.read_csv(download_file_to_memory(service, file['id']))
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° í° ê³µë°± ì²˜ë¦¬
        for column in df.select_dtypes(include=['object']).columns:
            # df[column] = df[column].str.replace(r'[^\w\s]', '', regex=True)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
            df[column] = df[column].str.replace(r'\s+', ' ', regex=True).str.strip()  # í° ê³µë°± ì œê±°
        dfs.append(df)

    merged_df = pd.concat(dfs, ignore_index=True)
    merged_df = merged_df.drop_duplicates(subset=['title'])  # title ì—´ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
    merged_df.to_csv('merged_data.csv', index=False)
    print("CSV files merged successfully with special character and whitespace handling.")

# CSV íŒŒì¼ ë¡œë“œ ë° í…ìŠ¤íŠ¸ ë¶„í• 
def load_and_split_csv(file_path, column_name="content", chunk_size=2000, chunk_overlap=200):
    loader = CSVLoader(file_path=file_path, source_column=column_name, encoding="utf-8")
    pages = loader.load_and_split()
    
    # 1. ë¬¸ì¥ ê¸°ë°˜ ë¶„í• ì„ ìœ„í•œ SpacyTextSplitter ì‚¬ìš© (ì˜ë¯¸ì  ë¶„í• )
    try:
        from langchain_text_splitters import SpacyTextSplitter
        # í•œêµ­ì–´ ëª¨ë¸ ì‚¬ìš©
        text_splitter = SpacyTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            pipeline="ko_core_news_sm"  # í•œêµ­ì–´ ëª¨ë¸
        )
        print("SpacyTextSplitterë¥¼ ì‚¬ìš©í•œ ì˜ë¯¸ì  ì²­í¬í™” ì ìš©")
    except Exception as e:
        print(f"SpacyTextSplitter ì´ˆê¸°í™” ì‹¤íŒ¨: {e}.")
        # 2. ëŒ€ì²´ ë°©ë²•: ë¬¸ë‹¨ ê¸°ë°˜ ë¶„í• 
        try:
            from langchain_text_splitters import NLTKTextSplitter
            text_splitter = NLTKTextSplitter(
                chunk_size=chunk_size, 
                chunk_overlap=chunk_overlap
            )
            print("NLTKTextSplitterë¥¼ ì‚¬ìš©í•œ ì˜ë¯¸ì  ì²­í¬í™” ì ìš©")
        except Exception as e:
            print(f"NLTKTextSplitter ì´ˆê¸°í™” ì‹¤íŒ¨: {e}. ê¸°ë³¸ ë¶„í• ê¸° ì‚¬ìš©")
            # 3. ê¸°ë³¸ RecursiveCharacterTextSplitter ì‚¬ìš©
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size, 
                chunk_overlap=chunk_overlap,
                separators=["\n\n", "\n", ".", " ", ""]  # ê°œí–‰, ë¬¸ì¥, ê³µë°± ìˆœìœ¼ë¡œ ë¶„í•  ì‹œë„
            )
            print("ê¸°ë³¸ RecursiveCharacterTextSplitter ì‚¬ìš©")
    
    docs = text_splitter.split_documents(pages)
    return docs

# ë²¡í„° ì €ì¥ì†Œ ìƒì„±
def create_vectorstore(docs):
    model_name = "jhgan/ko-sbert-nli"
    model_kwargs = {'device': 'cuda'}
    encode_kwargs = {'normalize_embeddings': True}

    embedding = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )

    # Chroma ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
    vectorstore = Chroma.from_documents(
        docs,
        embedding,
        persist_directory="./chroma_db"  # settings ëŒ€ì‹  persist_directory ì‚¬ìš©
    )
    return vectorstore.as_retriever()

# í…ìŠ¤íŠ¸ í˜•ì‹í™”
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# ì„ë² ë”© ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í‰ê°€ (ì—¬ëŸ¬ ë¬¸ë§¥ ì‚¬ìš©)
def evaluate_with_embedding(output, contexts):
    """
    ì¶œë ¥ ê²°ê³¼ì™€ ì—¬ëŸ¬ ë¬¸ë§¥(contexts)ì˜ ì„ë² ë”© ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    model_name = "jhgan/ko-sbert-nli"
    embedding_model = HuggingFaceEmbeddings(model_name=model_name)

    # ì¶œë ¥ ê²°ê³¼ ì„ë² ë”© ê³„ì‚°
    output_embedding = embedding_model.embed_query(output)

    # ê° ë¬¸ë§¥ì˜ ì„ë² ë”© ê³„ì‚° ë° ìœ ì‚¬ë„ ì¸¡ì •
    similarities = []
    for context in contexts:
        context_embedding = embedding_model.embed_query(context)
        similarity = cosine_similarity(
            [output_embedding], [context_embedding]
        )[0][0]
        similarities.append(similarity)

    # ê°€ì¥ ë†’ì€ ìœ ì‚¬ë„ë¥¼ ë°˜í™˜
    return max(similarities) if similarities else 0.0

# RAG ì²´ì¸ ì„¤ì • ë° ì‹¤í–‰
def run_rag_chain(retriever, query):
    # ì»¤ìŠ¤í…€ RAG í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
    template = """ë‹¤ìŒ ì ˆì°¨ì— ë”°ë¼ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. 

    1. ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì§ˆë¬¸ì— ê´€ë ¨ëœ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    2. ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ì •ë³´ê°€ ìˆìœ¼ë©´, ê·¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ì‘ì„±í•©ë‹ˆë‹¤.
    3. ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ëŠ” ë‚´ìš©ì´ë¼ë©´, "ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ì—ì„œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•©ë‹ˆë‹¤.

    ì»¨í…ìŠ¤íŠ¸:
    {context}

    ì§ˆë¬¸:
    {question}

    ë‹µë³€:
    """
    prompt = ChatPromptTemplate.from_template(template)
    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | ChatOllama(model="EEVE-Korean-10.8B:latest")
        | StrOutputParser()
    )

    # ì¶œë ¥ ê²°ê³¼ ì €ì¥ ë° ë¬¸ë§¥ ê°€ì ¸ì˜¤ê¸°
    output = ""
    context_docs = retriever.get_relevant_documents(query)
    contexts = [doc.page_content for doc in context_docs]
    for chunk in rag_chain.stream(query):
        output += chunk

    # ì—¬ëŸ¬ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì„ë² ë”© ìœ ì‚¬ë„ ê³„ì‚°
    score = evaluate_with_embedding(output, contexts)

    return output, score, contexts

def keyword_search(query, merged_data):
    """
    í‚¤ì›Œë“œ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ í•¨ìˆ˜
    """
    # ê²€ìƒ‰ì–´ë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜
    query = query.lower()
    keywords = query.split()
    
    # ê° ì»¬ëŸ¼ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰
    mask = merged_data['title'].str.lower().str.contains('|'.join(keywords), na=False) | \
           merged_data['content'].str.lower().str.contains('|'.join(keywords), na=False) | \
           merged_data['summary'].str.lower().str.contains('|'.join(keywords), na=False)
    
    return merged_data[mask]

def main():
    # Streamlit ë ˆì´ì•„ì›ƒ ì„¤ì •
    st.set_page_config(
        page_title="NASDAQ RAG Chatbog",
        page_icon="ğŸ¤–",
        layout="wide"  # ë³¸ë¬¸ì„ ë„“ê²Œ ì„¤ì •
    )

    st.title("ğŸ¤– NASDAQ RAG Chatbog")

    # êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì¸ì¦ ë° CSV íŒŒì¼ ì²˜ë¦¬
    FOLDER_ID = os.getenv('FOLDER_ID')
    service = authenticate_drive()
    csv_files = get_files(service, FOLDER_ID, 'text/csv')

    # ë³‘í•© ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„± ìƒíƒœë¥¼ ì¶”ì í•˜ê¸° ìœ„í•œ session_state ì„¤ì •
    if 'csv_merged' not in st.session_state:
        st.session_state['csv_merged'] = False
    if 'vectorstore_created' not in st.session_state:
        st.session_state['vectorstore_created'] = False

    # ì‚¬ì´ë“œë°”ì— CSV íŒŒì¼ ëª©ë¡ì„ ì—°ë„, ì›”ë³„ íŠ¸ë¦¬ í˜•ì‹ìœ¼ë¡œ í‘œì‹œ
    st.sidebar.title("ğŸ“‚ CSV íŒŒì¼ ëª©ë¡ & ë‹¤ìš´ë¡œë“œ")
    if csv_files:
        file_tree = {}

        # íŒŒì¼ ì´ë¦„ì—ì„œ ë‚ ì§œ ì¶”ì¶œ ë° íŠ¸ë¦¬ êµ¬ì¡° ìƒì„±
        for file in csv_files:
            file_name = file['name']
            file_id = file['id']
            file_content = download_file_to_memory(service, file_id).getvalue()

            # ë‚ ì§œ ì¶”ì¶œ (ì˜ˆ: 2023-04-10 í˜•ì‹)
            date_match = re.search(r'(\d{4})-(\d{2})-(\d{2})', file_name)
            if date_match:
                year, month, _ = date_match.groups()
                if year not in file_tree:
                    file_tree[year] = {}
                if month not in file_tree[year]:
                    file_tree[year][month] = []
                file_tree[year][month].append((file_name, file_id, file_content))

        # íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ ì‚¬ì´ë“œë°”ì— í‘œì‹œ (st.session_state ì œê±°)
        for year, months in sorted(file_tree.items()):
            with st.sidebar.expander(f"ğŸ“ {year}"):
                for month, files in sorted(months.items()):
                    st.markdown(f"### ğŸ“‚ {month}ì›”")
                    for file_name, file_id, file_content in files:
                        st.download_button(
                            label=f"ğŸ“¥ {file_name}",
                            data=file_content,
                            file_name=file_name,
                            mime="text/csv",
                            key=f"download-{file_id}"  # ê³ ìœ í•œ í‚¤ ì¶”ê°€
                        )
    else:
        st.sidebar.warning("ğŸš¨ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    if csv_files and not st.session_state['csv_merged']:
        with st.spinner("â³ CSV íŒŒì¼ ë³‘í•© ì¤‘..."):
            progress_bar = st.progress(0)
            total_files = len(csv_files)
            for i, file in enumerate(csv_files):
                # ì‹¤ì œ ì²˜ë¦¬ ë¡œì§ (ì˜ˆ: íŒŒì¼ ì½ê¸° ë° ë³‘í•©)
                pd.read_csv(download_file_to_memory(service, file['id']))
                progress = int(((i + 1) / total_files) * 100)  # ì§„í–‰ë¥  ê³„ì‚°
                progress_bar.progress(progress)
            merge_csv(csv_files, service)
        st.success("âœ… CSV íŒŒì¼ì´ ë³‘í•©ë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.session_state['csv_merged'] = True

    # ë³‘í•©ëœ CSV íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°
    if st.session_state['csv_merged'] and not st.session_state['vectorstore_created']:
        try:
            merged_data = pd.read_csv("merged_data.csv")
            st.subheader("ğŸ“Š ë³‘í•©ëœ CSV ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
            st.dataframe(merged_data[['title', 'date', 'summary']].head())  # title, date, summary ì—´ë§Œ ìƒìœ„ 10ê°œ í–‰ í‘œì‹œ
        except Exception as e:
            st.error(f"ğŸš¨ ë³‘í•©ëœ CSV íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

        # CSV íŒŒì¼ ë¡œë“œ ë° ë²¡í„°í™”
        try:
            with st.spinner("â³ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘..."):
                docs = load_and_split_csv("merged_data.csv")
                progress_bar = st.progress(0)
                for i in range(2):  # ë²¡í„°í™” ì‹œë®¬ë ˆì´ì…˜
                    time.sleep(0.5)
                    progress_bar.progress((i + 1) * 50)
                retriever = create_vectorstore(docs)
                st.session_state['retriever'] = retriever  # retrieverë¥¼ session_stateì— ì €ì¥
            st.success("âœ… ë²¡í„° ì €ì¥ì†Œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.session_state['vectorstore_created'] = True
        except Exception as e:
            st.error(f"ğŸš¨ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            return  # ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì‹¤íŒ¨ ì‹œ ì‹¤í–‰ ì¤‘ë‹¨

    if st.session_state['vectorstore_created']:
        retriever = st.session_state.get('retriever', None)
        if retriever is None:
            st.error("ğŸš¨ RAG ì²´ì¸ì„ ì‹¤í–‰í•˜ê¸° ìœ„í•´ retrieverê°€ í•„ìš”í•©ë‹ˆë‹¤. ë²¡í„° ì €ì¥ì†Œ ìƒì„± ê³¼ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
            return

        # ê²€ìƒ‰ ëª¨ë“œ ì„ íƒ
        search_mode = st.radio("ê²€ìƒ‰ ëª¨ë“œ ì„ íƒ", ["ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰", "í‚¤ì›Œë“œ ê²€ìƒ‰"])
        
        query = st.text_input("ğŸ’¬ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”:")

        if st.button("ğŸš€ ê²€ìƒ‰ ì‹¤í–‰"):
            if query:
                if search_mode == "í‚¤ì›Œë“œ ê²€ìƒ‰":
                    try:
                        merged_data = pd.read_csv("merged_data.csv")
                        results = keyword_search(query, merged_data)
                        if not results.empty:
                            st.success(f"âœ… {len(results)}ê°œì˜ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
                            for _, row in results.iterrows():
                                with st.expander(f"ğŸ“„ {row['title']} ({row['date']})"):
                                    st.write("**ìš”ì•½:**", row['summary'])
                                    st.write("**ë‚´ìš©:**", row['content'][:500] + "...")
                        else:
                            st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    except Exception as e:
                        st.error(f"ğŸš¨ í‚¤ì›Œë“œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                else:
                    # ê¸°ì¡´ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ë¡œì§
                    with st.spinner("â³ RAG ì²´ì¸ì„ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤..."):
                        try:
                            output, score, contexts = run_rag_chain(retriever, query)
                            st.success("âœ… RAG ì²´ì¸ ì‹¤í–‰ ì™„ë£Œ!")
                            st.text_area("ğŸ“œ ì¶œë ¥ ê²°ê³¼", output, height=min(400, max(100, len(output) // 2)))
                            st.write(f"**[ğŸŒŸ ì„ë² ë”© ìœ ì‚¬ë„]:** {score:.3f}")

                            # í”¼ë“œë°± ë²„íŠ¼ ì¶”ê°€
                            col1, col2 = st.columns(2)
                            with col1:
                                if st.button("ğŸ‘ ìœ ìš©í•¨", key="positive_feedback"):
                                    save_feedback(query, output, "positive")
                                    st.success("í”¼ë“œë°±ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
                            with col2:
                                if st.button("ğŸ‘ ìœ ìš©í•˜ì§€ ì•ŠìŒ", key="negative_feedback"):
                                    save_feedback(query, output, "negative")
                                    st.warning("í”¼ë“œë°±ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")

                            # ë¬¸ë§¥ í•˜ì´ë¼ì´íŠ¸ ì¶”ê°€
                            st.write("ğŸ” ê´€ë ¨ ë¬¸ë§¥:")
                            for context in contexts:
                                highlighted_context = context.replace(query, f"**{query}**")  # ê°„ë‹¨í•œ í•˜ì´ë¼ì´íŠ¸
                                st.markdown(f"{highlighted_context}")

                        except Exception as e:
                            st.error(f"ğŸš¨ RAG ì²´ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            else:
                st.error("ğŸš¨ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        retriever = None

if __name__ == '__main__':
    main()
